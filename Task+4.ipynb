{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().magic('matplotlib inline')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "data=pd.read_excel('caschool.xlsx.xls')\n",
    "\n",
    "state=440232650+470353886+470352982 # sum of the student IDs for the members of the group\n",
    "\n",
    "train = data.sample(frac=0.8, random_state=state) # For tasks 1-5 \n",
    "test = data[data.index.isin(train.index)==False].copy() # Only for prediction (task 6)\n",
    "\n",
    "train=train.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the proposed models we will want: \n",
    "1. OLS\n",
    "2. VIF\n",
    "3. SER\n",
    "4. $R^2-adj$ \n",
    "5. Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a log of a variable/predictor:\n",
    "train['log_testscr or variable'] = np.log(train['testscr or variable'])\n",
    "\n",
    "# to square-root Y:\n",
    "train['testscr_sqr'] = np.sqrt(train['testscr'])\n",
    "\n",
    "# to square Y:\n",
    "train['testscr_sq2'] = train['testscr']**2\n",
    "\n",
    "# creating knots for linear splines - this example splits at 20,40,60 and 80%:\n",
    "xi1=train['testscr'].quantile(.2) \n",
    "xi2=train['testscr'].quantile(.4)\n",
    "xi3=train['testscr'].quantile(.6)\n",
    "xi4=train['testscr'].quantile(.8)\n",
    "train['Step1']=(train['testscr']>xi1)*(train['testscr']-xi1)\n",
    "train['Step2']=(train['testscr']>xi2)*(train['testscr']-xi2)\n",
    "train['Step3']=(train['testscr']>xi3)*(train['testscr']-xi3)\n",
    "train['Step4']=(train['testscr']>xi4)*(train['testscr']-xi4)\n",
    "\n",
    "# creating cubic splines using knot positions defined above\n",
    "train['Step31']=(train['testscr']>xi1)*(train['testscr']-xi1)**3\n",
    "train['Step32']=(train['testscr']>xi2)*(train['testscr']-xi2)**3\n",
    "train['Step33']=(train['testscr']>xi3)*(train['testscr']-xi3)**3\n",
    "train['Step34']=(train['testscr']>xi4)*(train['testscr']-xi4)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. OLS\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "# insert formula you want, str must always be included as a variable\n",
    "\n",
    "# for interation effects use 'C:(variable1, variable2)'\n",
    "\n",
    "# to create a polynomial use '+ np.power(EngDispl, 2)' where 2 is the degree you're raising the variable to \n",
    "formula='testscr ~ str + avginc + el_pct + expn_stu + comp_stu'\n",
    "ols1 = smf.ols(formula=formula, data=train).fit()\n",
    "resid1 = ols1.resid\n",
    "fitted1 = ols1.fittedvalues\n",
    "ols1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. VIF\n",
    "\n",
    "features = train[['str','avginc','el_pct', 'expn_stu','comp_stu']] # add in all varibles for current model\n",
    "features = sm.add_constant(features)  # make sure to include a column of 1s when using the variance inflation factor function.\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = []\n",
    "for i in range(6): # range is number of selected variables + 1\n",
    "    vif.append(variance_inflation_factor(features.values, i+1))\n",
    "    \n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average VIF\n",
    "np.mean(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. and 4.  SER, Rsq and Rsq adj\n",
    "\n",
    "# For a linear Y:\n",
    "ols1.mse_resid**0.5 # SER\n",
    "\n",
    "    # Rsq and Rsq Adj are in the OLS summary\n",
    "\n",
    "# For logY:\n",
    "    # can do this without bias correction (BC), Duan BC (non-normal errors), or Normal BC (normal errors)\n",
    "    # this example assumes a loglin model, its the same with a loglog model\n",
    "\n",
    "eres1 = np.exp(resid1) # exponential of errors\n",
    "n = len(resid1)\n",
    "fp1 = sum(eres1)/n # Duan BC factor\n",
    "fp11 = np.exp(ols1.mse_resid/2) # Normal BC factor\n",
    "\n",
    "    # untransform Y \n",
    "testscr_loglin = np.exp(fitted1) # no BC\n",
    "testscr_loglin1 = np.exp(fitted1) * fp1 # Duan\n",
    "testscr_loglin11 = np.exp(fitted1) * fp11 # Normal\n",
    "\n",
    "    # get variance for calculating SER, R2 etc, it will never change\n",
    "stats.describe(train['testscr'])\n",
    "\n",
    "    # calculate new residuals and then SER, Rsq and Rsq adj for each BC, p is the number of predictors\n",
    "res_loglin = train['testscr']-testscr_loglin\n",
    "np.sqrt(sum(res_loglin**2)/(n-p-1)), 1 - sum(res_loglin**2)/((n-1)*variance), 1 - sum(res_loglin**2)/(n-p-1)/variance\n",
    "\n",
    "res_loglin1 = train['testscr']-testscr_loglin1\n",
    "np.sqrt(sum(res_loglin1**2)/(n-p-1)), 1 - sum(res_loglin1**2)/((n-1)*variance), 1 - sum(res_loglin1**2)/(n-p-1)/variance\n",
    "\n",
    "res_loglin11 = train['testscr']-testscr_loglin11\n",
    "np.sqrt(sum(res_loglin11**2)/(n-p-1)), 1 - sum(res_loglin11**2)/((n-1)*variance), 1 - sum(res_loglin11**2)/(n-p-1)/variance\n",
    "\n",
    "\n",
    "# For square-root Y\n",
    "testscr_pred_sq2 = fitted1**2 #square the fitted values to get Y\n",
    "\n",
    "    # find residuals then SER, R2, R2 adj, p is the number of predictors\n",
    "res_pred_sq2 = train['testscr']-testscr_pred_sq2\n",
    "np.sqrt(sum(res_pred_sq2**2)/(n-p-1)), 1 - sum(res_pred_sq2**2)/((n-1)*variance), 1 - sum(res_pred_sq2**2)/(n-p-1)/variance\n",
    "\n",
    "# for Y^2 \n",
    "testscr_pred_sqr = np.sqrt(fitted1) #square root the fitted values to get Y\n",
    "    # find residuals then SER, R2, R2 adj\n",
    "res_pred_sqr = train['testscr']-testscr_pred_sqr\n",
    "np.sqrt(sum(res_pred_sqr**2)/(n-p-1)), 1 - sum(res_pred_sqr**2)/((n-1)*variance), 1 - sum(res_pred_sqr**2)/(n-p-1)/variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Residuals, gives LOESS, true line\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "sns.regplot(fitted1, resid1, lowess=True, ax=ax, scatter_kws={'s': 35, 'alpha': .6}) # remember to use untransformed fitted values if you transformed Y\n",
    "ax.set_xlabel('Fitted',  {'fontsize': 12})\n",
    "ax.set_ylabel('Residuals', {'fontsize': 12})\n",
    "ax.set_title('Residuals vs Fitted - Model X') # title = current model\n",
    "plt.axhline(color='Black', alpha=0.3, linestyle='--')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram of residuals (to check whether the distrubution is uniform/normal)\n",
    "\n",
    "ax = sns.distplot(resid1, bins=30)\n",
    "ax.set_xlabel('Residuals',  {'fontsize': 12})\n",
    "ax.set_ylabel('Frequency', {'fontsize': 12})\n",
    "ax.set_title('Risdual Histogram - Model X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numerical summary of residuals\n",
    "from scipy import stats\n",
    "stats.describe(resid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gives a graph of the residuals squared with a true fit line (good for checking homoskedasticity)\n",
    "tableau=['#1F77B4', '#FF7F0E', '#2CA02C', '#DB2728', '#9467BD', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "fig, ax= plt.subplots()\n",
    "sns.regplot(fitted1,resid1**2, ci=None, fit_reg=False, scatter_kws={'s': 35, 'color': tableau[3], 'alpha': 0.7})\n",
    "ax.set_xlabel('Fitted values',  {'fontsize': 12})\n",
    "ax.set_ylabel('Residual squared', {'fontsize': 12})\n",
    "z1 = lowess(resid**2, fitted1, frac=1./10)\n",
    "plt.plot(z1[:,0],z1[:,1],'blue')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
